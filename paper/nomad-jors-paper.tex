\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}

\title{Software paper for submission to the\\Journal of Open Research Software}
\author{}
\date{}

\begin{document}

\maketitle

\section*{(1) Overview}

\subsection*{Title}
NØMAD: Lightweight HPC Monitoring and Diagnostics with Machine Learning-Based Failure Prediction

\subsection*{Paper Authors}
\begin{enumerate}
\item Tonini, João Filipe Riva (corresponding author)
\end{enumerate}

\subsection*{Paper Author Roles and Affiliations}
\begin{enumerate}
\item Academic Research Computing, University of Richmond, Richmond, VA 23173, USA.\\
Email: jtonini@richmond.edu\\
ORCID: 0000-0002-4730-3805
\end{enumerate}

\subsection*{Abstract}
NØMAD (NOde Monitoring And Diagnostics) is a lightweight monitoring and predictive analytics tool for computing infrastructure. It collects system metrics (disk, CPU, memory, I/O, GPU) from any Linux system, with optional SLURM \cite{slurm} integration for job-level analytics. Data is stored in SQLite, and machine learning models predict job failures before they occur. A key innovation is modeling jobs as nodes in a similarity network, where connections represent shared resource usage patterns. Jobs with similar characteristics often share similar outcomes, and the network structure reveals failure-prone regions in the feature space. The tool provides a real-time web dashboard and supports alerts via email, Slack, or webhooks. NØMAD requires no external databases or complex infrastructure, making it suitable for small-to-medium HPC centers and research groups.

\subsection*{Keywords}
HPC; high-performance computing; monitoring; machine learning; SLURM; predictive analytics; failure prediction

\subsection*{Introduction}

HPC administrators face a persistent challenge: detecting job failures
before they impact researchers. Traditional monitoring approaches fall
into two categories: enterprise IT solutions and HPC-specific tools,
each with distinct trade-offs.

Enterprise monitoring solutions like Prometheus \cite{prometheus},
Grafana \cite{grafana}, and Nagios \cite{nagios} provide mature, feature-rich platforms but require
significant infrastructure---database servers, message queues, and
dedicated hardware---that may be impractical for small-to-medium HPC
centers. These tools target general IT workloads and lack native
understanding of HPC concepts such as job schedulers, batch queues, and
compute allocations.

HPC-specific tools address this domain gap but introduce their own
complexity. TACC Stats \cite{tacc} provides comprehensive hardware
counter data and correlates system metrics with job performance,
enabling detailed post-hoc analysis of application behavior. XDMoD
\cite{xdmod} offers institutional-scale job accounting and resource
utilization reporting with sophisticated visualization capabilities, but
its deployment involves multiple database backends and web services. The
Lightweight Distributed Metric Service (LDMS) \cite{ldms} achieves
impressive scalability for continuous monitoring on large systems
through its lightweight, distributed architecture. These tools excel at
retrospective analysis---understanding what happened after a
failure---but provide limited support for real-time prediction of
failures before they occur.

A complementary approach treats failure prediction as a pattern
recognition problem. Jobs that fail often share common characteristics:
excessive NFS writes, memory pressure, or inefficient CPU utilization.
Machine learning methods have been applied to HPC failure prediction
\cite{mlhpc}, typically using classification models trained on job
features. However, these approaches often treat jobs as independent
observations, ignoring the structural relationships between jobs with
similar resource profiles.

NØMAD addresses these gaps by combining zero-infrastructure deployment
with network-based failure prediction. The key insight is that jobs with
similar resource usage patterns tend to experience similar outcomes.
Rather than treating each job independently, NØMAD models jobs as nodes
in a similarity network where edges connect jobs with comparable
behavioral fingerprints. This network structure reveals failure-prone
regions in the feature space---clusters of jobs that share
characteristics associated with poor outcomes---enabling both risk
assessment for individual jobs and actionable recommendations for users.

The choice of similarity metric is consequential for network topology.
NØMAD uses cosine similarity on Z-score normalized feature vectors,
which emphasizes the \emph{shape} of resource usage profiles rather than
absolute magnitudes. A job requesting 64GB of memory with 50\%
utilization has a similar profile to one requesting 8GB with 50\%
utilization---both represent reasonable memory sizing---even though
their absolute consumption differs by an order of magnitude. This
approach draws inspiration from network-based methods in biogeography,
where similarity metrics identify emergent regions from species
distribution patterns \cite{vilhena}. Just as biogeographical networks
reveal ecological boundaries from observational data without predefined
regions, job similarity networks reveal behavioral boundaries between
successful and failing workloads without predefined failure categories.
The similarity threshold (default $\geq 0.7$) controls network density:
higher thresholds create sparser networks with tighter clusters, while
lower thresholds reveal broader patterns at the cost of specificity.

NØMAD provides: (1) zero-infrastructure deployment using a single
SQLite database with no external services; (2) system-level monitoring
that works on any Linux system without requiring HPC software; (3)
optional SLURM integration for job-level analytics and predictive
alerts; (4) similarity network analysis that learns failure patterns
from historical data; and (5) educational analytics that track the
development of computational proficiency over time, enabling insights
such as ``15 of 20 students improved memory efficiency over the
semester.''

\subsection*{Implementation and Architecture}
NØMAD is implemented in Python and follows a modular architecture with four main components (Figure~\ref{fig:architecture}):

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/architecture.png}
\caption{NØMAD system architecture showing the data flow from collectors through the analysis engines to the alert dispatcher and web dashboard. The monitoring engine handles threshold-based alerts while the prediction engine uses ML models for proactive failure detection.}
\label{fig:architecture}
\end{figure}

\textbf{Collectors} gather metrics from standard Linux tools (\texttt{iostat}, \texttt{vmstat}, \texttt{nvidia-smi}, \texttt{nfsiostat}) and filesystem utilities. When SLURM is available, additional collectors provide queue state (\texttt{squeue}, \texttt{sinfo}), job history (\texttt{sacct}), and per-job I/O statistics from \texttt{/proc/[pid]/io}. Collectors gracefully skip when their requirements are not met (e.g., no GPU monitoring without \texttt{nvidia-smi}).

\textbf{Feature Engineering} transforms raw metrics into a feature vector per job. System-level features include I/O wait, memory pressure, swap activity, and device utilization from \texttt{iostat}, \texttt{mpstat}, and \texttt{vmstat}. When SLURM is available, job-specific features (CPU/memory efficiency, NFS write ratios, runtime) extend the vector to 17 dimensions. These features enable similarity-based analysis across jobs using cosine similarity.

\textbf{ML Prediction} uses an ensemble of three models: a Graph Neural Network (GNN) that captures relationships between similar jobs based on cosine similarity of feature vectors; an LSTM that detects temporal patterns and early warning trajectories; and an Autoencoder that identifies anomalous jobs deviating from normal behavior. The ensemble outputs a continuous risk score (0--1) rather than binary classification.

\textbf{Alert System} supports both threshold-based alerts (disk usage, GPU temperature) and predictive alerts using derivative analysis. When the rate of change indicates a threshold will be breached, alerts fire before the actual breach occurs. Notifications route through email, Slack, or webhooks with configurable cooldowns.

\subsection*{Web Dashboard}
NØMAD provides a real-time web dashboard for monitoring cluster health and job status (Figure~\ref{fig:dashboard}). The dashboard displays partition-specific metrics including queue depth, node utilization, and resource consumption patterns.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/dashboard.png}
\caption{NØMAD web dashboard showing real-time cluster status. Top left displays compute partition metrics, top right shows high-memory partition status, bottom left presents GPU partition health, and bottom right visualizes the job similarity network. The interface provides at-a-glance monitoring for HPC administrators.}
\label{fig:dashboard}
\end{figure}

The dashboard includes partition-specific views that allow administrators to monitor different resource types independently (Figure~\ref{fig:partitions}). Each partition view displays relevant metrics: CPU utilization and memory pressure for compute partitions, GPU temperature and VRAM usage for GPU partitions, and memory allocation patterns for high-memory partitions.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{figures/compute.png}

\vspace{0.2cm}
\includegraphics[width=0.75\textwidth]{figures/gpu.png}

\vspace{0.2cm}
\includegraphics[width=0.75\textwidth]{figures/highmem.png}
\caption{Partition-specific dashboard views showing compute (top, 6-node partition with CPU utilization), GPU (middle, 2-node partition with GPU metrics), and high-memory (bottom, 2-node partition optimized for memory-intensive jobs). Each view displays node health rings, job statistics, and resource utilization bars specific to the partition type.}
\label{fig:partitions}
\end{figure}

\subsection*{Job Similarity Network}
A distinguishing feature of NØMAD is the job similarity network visualization (Figure~\ref{fig:network}). Jobs are represented as nodes in a 3D space defined by their I/O characteristics: NFS write ratio, local write volume, and I/O wait percentage. When full job metrics are available, edges connect jobs with cosine similarity $\geq 0.7$ based on their feature vectors.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/network.png}
\caption{Three-dimensional job similarity network visualization. Jobs are positioned by their I/O behavior (NFS writes, local writes, I/O wait) and colored by health score (green = healthy, red = failed). Clustering reveals failure patterns: jobs in the high-NFS, high-I/O-wait region show elevated failure rates compared to jobs using local storage.}
\label{fig:network}
\end{figure}

This visualization reveals how failures cluster in specific regions of the feature space. Jobs in the ``danger zone'' (high NFS ratio, high I/O wait) show significantly higher failure rates than jobs in the ``safe zone'' (low NFS ratio, high local writes). Because these patterns emerge from historical data rather than predefined rules, the system can discover site-specific failure modes and provide targeted recommendations to users (e.g., ``jobs with your I/O pattern have a 72\% failure rate---consider using local scratch instead of NFS'').

\subsection*{Quality Control}
NØMAD includes a comprehensive test suite covering unit tests for collectors, feature engineering, and ML components; integration tests for the full data pipeline; a demo mode (\texttt{nomad demo}) that generates synthetic HPC data for testing without requiring a real cluster; and continuous integration via GitHub Actions.

The software has been tested on clusters ranging from 6 to 200 nodes, including both CPU-only and GPU-enabled partitions. The codebase follows PEP 8 style guidelines and includes type hints for improved maintainability.

\subsection*{Educational Analytics}

NØMAD includes an educational analytics module (\texttt{nomad edu}) designed
for classroom instruction, research mentorship, and HPC training programs. The
module tracks the development of computational proficiency over time by
analyzing per-job behavioral fingerprints across five dimensions: CPU
efficiency, memory sizing, time estimation, I/O awareness, and GPU utilization.

\textbf{Job Explanation} (\texttt{nomad edu explain}): Translates raw job data
into plain-language feedback. For each completed job, the system provides
dimension-specific scores (0--100), proficiency levels (Excellent, Good,
Developing, Needs Work), and actionable recommendations. For example, a job
with 21\% CPU utilization receives the feedback: ``Very low CPU utilization---
requested 4 cores but used $\sim$1. Try: \texttt{\#SBATCH --ntasks=1}.''

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{figures/edu_explain.png}\hspace{0.5cm}\includegraphics[width=0.45\textwidth]{figures/edu_trajectory.png}

\vspace{0.3cm}

\includegraphics[width=0.45\textwidth]{figures/edu_report.png}

\vspace{0.2cm}


\caption{Educational analytics outputs. \textbf{Top left} (\texttt{nomad edu explain 1104}): Job explanation
with proficiency scores and recommendations. \textbf{Top right} (\texttt{nomad edu trajectory alice}): User trajectory
tracking improvement over 173 jobs. \textbf{Bottom} (\texttt{nomad edu report cs101}): Group report for a course
section with per-student breakdown.}
\label{fig:edu}
\end{figure}

\textbf{User Trajectories} (\texttt{nomad edu trajectory}): Tracks an
individual's improvement across their job submissions. The system computes
sliding-window averages to identify trends (improving, stable, declining) for
each proficiency dimension, enabling mentors to provide targeted guidance.

\textbf{Group Reports} (\texttt{nomad edu report}): Aggregates proficiency
data across course sections or research groups. Instructors can generate
summaries such as ``15 of 20 students improved memory efficiency over the
semester'' or identify that a majority of students struggle with I/O patterns,
suggesting curriculum adjustments.

These features address a common challenge in HPC education: students submit
jobs that technically complete but exhibit significant resource waste or
suboptimal patterns. Traditional approaches require manual inspection of
\texttt{sacct} output, which is impractical for classes with hundreds of
students. NØMAD automates this assessment while providing pedagogically useful
feedback that helps students develop genuine computational proficiency rather
than simply learning to avoid errors.

Proficiency scores are persisted to the database, enabling longitudinal studies
of HPC training effectiveness and research into factors that accelerate skill
development.

\section*{(2) Availability}

\subsection*{Operating System}
Linux (tested on Ubuntu 22.04, Rocky Linux 9, CentOS 7)

\subsection*{Programming Language}
Python 3.9+

\subsection*{Additional System Requirements}
\begin{itemize}
\item SLURM workload manager (optional, enables job-level analytics)
\item sysstat package (\texttt{iostat}, \texttt{mpstat}) for system metrics
\item Network access for dashboard (default port 5000)
\item Minimal disk space for SQLite database ($\sim$1MB per 10,000 samples)
\end{itemize}

\subsection*{Dependencies}
\begin{itemize}
\item click $\geq$ 8.0
\item toml $\geq$ 0.10
\item numpy $\geq$ 1.21
\item pandas $\geq$ 1.3
\item scipy $\geq$ 1.7
\item scikit-learn \cite{sklearn} (optional, for ML features)
\item torch (optional, for GNN/LSTM models)
\item torch-geometric (optional, for graph neural networks)
\end{itemize}

\subsection*{List of Contributors}
\begin{enumerate}
\item Tonini, João Filipe Riva (Lead developer)
\end{enumerate}

\subsection*{Software Location}

\subsubsection*{Archive}
\begin{tabular}{ll}
Name: & Zenodo \\
Persistent identifier: & \url{https://doi.org/10.5281/zenodo.18614517} \\
Licence: & AGPL-3.0-or-later \\
Publisher: & João Filipe Riva Tonini \\
Version: & 1.2.2 \\
Date published: & February 2026 \\
\end{tabular}

\subsubsection*{Code Repository}
\begin{tabular}{ll}
Name: & GitHub \\
Identifier: & \url{https://github.com/jtonini/nomad-hpc} \\
Licence: & AGPL-3.0-or-later \\
Date published: & December 2025 \\
\end{tabular}

\subsection*{Language}
English

\section*{(3) Reuse Potential}

NØMAD is designed for broad reuse across HPC environments of varying scales. The software can be deployed in several contexts:

\textbf{Small Research Groups}: Groups managing individual workstations or small clusters can use NØMAD's zero-infrastructure design to implement sophisticated monitoring without enterprise tools. The \texttt{nomad demo} mode allows evaluation without requiring existing HPC infrastructure.

\textbf{Medium HPC Centers}: University and institutional HPC centers can deploy NØMAD as either a primary monitoring solution or as a complement to existing tools like Grafana or Nagios, adding predictive capabilities to traditional threshold-based alerting.

\textbf{Research Applications}: The similarity network approach provides a
framework for studying HPC failure patterns. The network structure, combined
with job-level proficiency scores, enables quantitative analysis of resource
usage patterns and their relationship to job outcomes.

\textbf{Community Data Sharing}: The \texttt{nomad community export} command
generates anonymized datasets suitable for cross-institutional research. Export
formats include:
\begin{itemize}
\item \textbf{Job fingerprints}: Feature vectors with hashed user/job identifiers
\item \textbf{Failure patterns}: Aggregated statistics on failure modes by resource profile
\item \textbf{Proficiency distributions}: Anonymized skill development trajectories
\end{itemize}
These exports enable collaborative research on HPC usage patterns without
exposing sensitive institutional data. Researchers can study questions such as:
Which failure modes are universal vs.\ site-specific? Do proficiency
improvement rates vary by discipline? What resource configurations correlate
with job success across different cluster architectures?

The anonymization process removes usernames, job names, and absolute timestamps
while preserving the relational structure needed for network-based analysis.
Exported data uses relative time offsets and hashed identifiers, making it
suitable for publication as supplementary materials or inclusion in shared
research datasets.

\textbf{Educational Use}: Beyond the demo mode, NØMAD provides a complete
educational analytics pipeline. Instructors can assign Linux groups to course
sections (e.g., \texttt{cs101}, \texttt{bio301}) and use \texttt{nomad edu
report} to track class-wide proficiency development. Individual students
receive automated feedback via \texttt{nomad edu explain}, reducing instructor
workload while providing immediate, actionable guidance. The module is
particularly valuable for:
\begin{itemize}
\item \textbf{HPC workshops}: Pre/post assessment of participant skills
\item \textbf{Research onboarding}: Tracking new graduate students' development
\item \textbf{Classroom instruction}: Automated grading of computational assignments
\item \textbf{Self-directed learning}: Students can independently review their job history
\end{itemize}

\subsection*{Extension Points}
Developers can extend NØMAD in several ways:
\begin{itemize}
\item \textbf{Custom Collectors}: Add new metric sources by implementing the collector interface
\item \textbf{Alert Channels}: Integrate additional notification services beyond email/Slack/webhooks
\item \textbf{ML Models}: Train site-specific models using the provided feature engineering pipeline
\item \textbf{Dashboard Widgets}: Extend the web dashboard with custom visualizations
\end{itemize}

\subsection*{Support}
\begin{itemize}
\item GitHub Issues: \url{https://github.com/jtonini/nomad-hpc/issues}
\item Email: jtonini@richmond.edu
\item Documentation: \url{https://jtonini.github.io/nomad-hpc/}
\end{itemize}

Users and developers are welcome to submit bug reports, feature requests, and pull requests via GitHub.

\section*{Acknowledgements}
The author thanks George Flanagin for advice and inspiration on HPC system administration, and the University of Richmond's Office of the Provost for financial and resource support.

\section*{Funding Statement}
This work was supported by the University of Richmond.

\section*{Competing Interests}
The author declares no competing interests.

\begin{thebibliography}{10}

\bibitem{slurm}
Yoo, A.B., Jette, M.A., Grondona, M. (2003). SLURM: Simple Linux Utility for Resource Management. In: Job Scheduling Strategies for Parallel Processing, Lecture Notes in Computer Science, vol. 2862, Springer, pp. 44--60. DOI: \url{https://doi.org/10.1007/10968987_3}

\bibitem{prometheus}
Prometheus Authors (2012--present). Prometheus: From metrics to insight.
URL: \url{https://prometheus.io/}

\bibitem{grafana}
Grafana Labs (2014--present). Grafana: The open observability platform.
URL: \url{https://grafana.com/}

\bibitem{nagios}
Galstad, E. (1999--present). Nagios: The industry standard in IT
infrastructure monitoring. URL: \url{https://www.nagios.org/}

\bibitem{tacc}
Evans, R.T., Browne, J.C., Barth, W.L. (2014). Comprehensive resource use monitoring for HPC systems with TACC Stats. In: Proceedings of the First International Workshop on HPC User Support Tools, IEEE, pp. 13--21. DOI: \url{https://doi.org/10.1109/SC.2014.18}

\bibitem{xdmod}
Palmer, J.T., Gallo, S.M., Furlani, T.R., Jones, M.D., DeLeon, R.L., White, J.P., Simakov, N., Patra, A.K., Sperhac, J., Yearke, T., Rathsam, R., Inber, M., Guillen, O., Cornelius, C.D. (2015). Open XDMoD: A tool for the comprehensive management of high-performance computing resources. Computing in Science \& Engineering 17(4):52--62. DOI: \url{https://doi.org/10.1109/MCSE.2015.32}

\bibitem{ldms}
Agelastos, A., Allan, B., Brandt, J., Cassella, P., Enos, J., Fullop, J., Gentile, A., Monk, S., Naksinehaboon, N., Ogden, J., Rajan, M., Showerman, M., Stevenson, J., Taerat, N., Tucker, T. (2014). The Lightweight Distributed Metric Service: A scalable infrastructure for continuous monitoring of large scale computing systems and applications. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, pp. 154--165. DOI: \url{https://doi.org/10.1145/2616498.2616533}

\bibitem{mlhpc}
Tuncer, O., Ates, E., Zhang, Y., Turber, A., Brandt, J., Leung, V.J.,
Egele, M., Coskun, A.K. (2017). Diagnosing performance variations in HPC
applications using machine learning. In: High Performance Computing,
Lecture Notes in Computer Science, vol. 10266, Springer, pp. 355--373.
DOI: \url{https://doi.org/10.1007/978-3-319-58667-0_19}

\bibitem{vilhena}
Vilhena, D.A., Antonelli, A. (2015). A network approach for identifying
and delimiting biogeographical regions. Nature Communications 6:6848.
DOI: \url{https://doi.org/10.1038/ncomms7848}

\bibitem{sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825--2830. URL: \url{http://jmlr.org/papers/v12/pedregosa11a.html}

\end{thebibliography}

\end{document}
