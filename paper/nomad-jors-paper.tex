\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}

\title{Software paper for submission to the\\Journal of Open Research Software}
\author{}
\date{}

\begin{document}

\maketitle

\section*{(1) Overview}

\subsection*{Title}
NØMAD: Lightweight HPC Monitoring and Diagnostics with Machine Learning-Based Failure Prediction

\subsection*{Paper Authors}
\begin{enumerate}
\item Tonini, João Filipe Riva (corresponding author)
\end{enumerate}

\subsection*{Paper Author Roles and Affiliations}
\begin{enumerate}
\item Academic Research Computing, University of Richmond, Richmond, VA 23173, USA.\\
Email: jtonini@richmond.edu\\
ORCID: 0000-0002-4730-3805
\end{enumerate}

\subsection*{Abstract}
NØMAD (NOde Monitoring And Diagnostics) is a lightweight monitoring and predictive analytics tool for computing infrastructure. Metric collectors run via systemd timers, gathering system metrics (disk, CPU, memory, I/O, GPU utilization and temperature) from standard Linux tools and storing them in SQLite. With optional SLURM \cite{slurm} integration, collectors also capture job-level analytics from scheduler commands and per-process I/O statistics. Machine learning models predict job failures before they occur; a data readiness estimator helps administrators determine when sufficient data has been collected for reliable predictions. Diagnostic tools enable targeted analysis of network performance, storage health, and node-level bottlenecks. A key innovation is modeling jobs as nodes in a similarity network, where connections represent shared resource usage patterns. Jobs with similar characteristics often share similar outcomes, and the network structure reveals failure-prone regions in the feature space. The tool provides a real-time web dashboard and supports alerts via email, Slack, or webhooks. NØMAD requires no external databases or complex infrastructure, making it suitable for small-to-medium HPC centers and research groups.

\subsection*{Keywords}
HPC; high-performance computing; monitoring; machine learning; SLURM; predictive analytics; failure prediction

\subsection*{Introduction}

HPC administrators face a persistent challenge: detecting job failures
before they impact researchers. Traditional monitoring approaches fall
into two categories: enterprise IT solutions and HPC-specific tools,
each with distinct trade-offs.

Enterprise monitoring solutions like Prometheus \cite{prometheus},
Grafana \cite{grafana}, and Nagios \cite{nagios} provide mature, feature-rich platforms but require
significant infrastructure---database servers, message queues, and
dedicated hardware---that may be impractical for small-to-medium HPC
centers. These tools target general IT workloads and lack native
understanding of HPC concepts such as job schedulers, batch queues, and
compute allocations.

HPC-specific tools address this domain gap but introduce their own
complexity. TACC Stats \cite{tacc} provides comprehensive hardware
counter data and correlates system metrics with job performance,
enabling detailed post-hoc analysis of application behavior. XDMoD
\cite{xdmod} offers institutional-scale job accounting and resource
utilization reporting with sophisticated visualization capabilities, but
its deployment involves multiple database backends and web services. The
Lightweight Distributed Metric Service (LDMS) \cite{ldms} achieves
impressive scalability for continuous monitoring on large systems
through its lightweight, distributed architecture. These tools excel at
retrospective analysis---understanding what happened after a
failure---but provide limited support for real-time prediction of
failures before they occur.

A complementary approach treats failure prediction as a pattern
recognition problem. Jobs that fail often share common characteristics:
excessive NFS writes, memory pressure, or inefficient CPU utilization.
Machine learning methods have been applied to HPC failure prediction
\cite{mlhpc}, typically using classification models trained on job
features. However, these approaches often treat jobs as independent
observations, ignoring the structural relationships between jobs with
similar resource profiles.

NØMAD addresses these gaps by combining zero-infrastructure deployment
with network-based failure prediction. The key insight is that jobs with
similar resource usage patterns tend to experience similar outcomes.
Rather than treating each job independently, NØMAD models jobs as nodes
in a similarity network where edges connect jobs with comparable
behavioral fingerprints. This network structure reveals failure-prone
regions in the feature space---clusters of jobs that share
characteristics associated with poor outcomes---enabling both risk
assessment for individual jobs and actionable recommendations for users.

The choice of similarity metric is consequential for network topology.
NØMAD uses cosine similarity on Z-score normalized feature vectors,
which emphasizes the \emph{shape} of resource usage profiles rather than
absolute magnitudes. A job requesting 64GB of memory with 50\%
utilization has a similar profile to one requesting 8GB with 50\%
utilization---both represent reasonable memory sizing---even though
their absolute consumption differs by an order of magnitude. This
approach draws inspiration from network-based methods in biogeography,
where similarity metrics identify emergent regions from species
distribution patterns \cite{vilhena}. Just as biogeographical networks
reveal ecological boundaries from observational data without predefined
regions, job similarity networks reveal behavioral boundaries between
successful and failing workloads without predefined failure categories.
The similarity threshold (default $\geq 0.7$) controls network density:
higher thresholds create sparser networks with tighter clusters, while
lower thresholds reveal broader patterns at the cost of specificity.

NØMAD provides: (1) zero-infrastructure deployment using a single
SQLite database with no external services; (2) system-level monitoring
that works on any Linux system without requiring HPC software; (3)
optional SLURM integration for job-level analytics and predictive
alerts; (4) similarity network analysis that learns failure patterns
from historical data; and (5) educational analytics that track the
development of computational proficiency over time, enabling insights
such as ``15 of 20 students improved memory efficiency over the
semester.''

\subsection*{Implementation and Architecture}
NØMAD is implemented in Python and follows a modular architecture with seven main components (Figure~\ref{fig:architecture}):

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/architecture.png}
\caption{NØMAD system architecture showing the data flow from collectors through the analysis engines to the alert dispatcher and web dashboard. The monitoring engine handles threshold-based alerts while the prediction engine uses ML models for proactive failure detection.}
\label{fig:architecture}
\end{figure}

\textbf{Collectors} are scheduled via systemd timers at configurable intervals (default: 60 seconds). They gather metrics from standard Linux tools (\texttt{iostat}, \texttt{vmstat}, \texttt{nvidia-smi}, \texttt{nfsiostat}) and filesystem utilities. When SLURM is available, additional collectors provide queue state (\texttt{squeue}, \texttt{sinfo}), job history (\texttt{sacct}), and per-job I/O statistics from \texttt{/proc/[pid]/io}. Collectors gracefully skip when their requirements are not met (e.g., no GPU monitoring without \texttt{nvidia-smi}).

\textbf{Feature Engineering} transforms raw metrics into a feature vector per job. System-level features include I/O wait, memory pressure, swap activity, and device utilization from \texttt{iostat}, \texttt{mpstat}, and \texttt{vmstat}. When SLURM is available, job-specific features (CPU/memory efficiency, NFS write ratios, runtime) extend the vector to 17 dimensions. These features enable similarity-based analysis across jobs using cosine similarity.

\textbf{ML Prediction} uses an ensemble of three models: a Graph Neural Network (GNN) that captures relationships between similar jobs based on cosine similarity of feature vectors; an LSTM that detects temporal patterns and early warning trajectories; and an Autoencoder that identifies anomalous jobs deviating from normal behavior. The ensemble outputs a continuous risk score (0--1) rather than binary classification.

\textbf{Alert System} supports both threshold-based alerts (disk usage, GPU temperature) and predictive alerts using derivative analysis. When the rate of change indicates a threshold will be breached, alerts fire before the actual breach occurs. Notifications route through email, Slack, or webhooks with configurable cooldowns.

\textbf{Data Readiness Estimator} helps administrators determine when sufficient data has been collected for reliable ML predictions. The estimator assesses sample size adequacy (minimum 100 jobs, recommended 500, optimal 1000+), class balance between successful and failed jobs, feature coverage and variance, and data recency. Based on current collection rates, it forecasts time-to-readiness: ``At 125 jobs/day, recommended threshold will be reached in approximately 3 days.'' The command \texttt{nomad readiness} produces a detailed report with scores for each dimension and actionable recommendations.

\textbf{Diagnostics Module} provides targeted analysis for common HPC issues. The \texttt{nomad diag} commands analyze network performance between nodes (\texttt{nomad diag network}), storage health and I/O patterns (\texttt{nomad diag storage}), and node-level resource bottlenecks (\texttt{nomad diag node}). Each diagnostic produces both summary statistics and detailed drill-down capabilities, helping administrators quickly identify root causes of performance issues.

\textbf{Infrastructure Monitoring} extends beyond compute nodes to cover research workstations and storage systems. The dashboard includes dedicated views for departmental workstations (tracking CPU, memory, disk usage, and logged-in users) and storage servers (monitoring ZFS pool health, NFS client connections, IOPS, throughput, and latency). This holistic view enables administrators to correlate job failures with infrastructure-wide issues such as NFS server congestion or storage capacity constraints.


\subsection*{Web Dashboard}
NØMAD provides a real-time web dashboard for monitoring cluster health and job status (Figure~\ref{fig:dashboard}). The dashboard displays partition-specific metrics including queue depth, node utilization, and resource consumption patterns.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/dashboard.png}
\caption{NØMAD web dashboard in light theme showing real-time cluster status. The main view displays three partitions (compute, highmem, gpu) with node health rings indicating CPU utilization. The sidebar shows detailed statistics for the selected node including job counts, resource utilization, and top users.}
\label{fig:dashboard}
\end{figure}

The dashboard provides additional analytics views for resource tracking and infrastructure monitoring. The usage analytics view (Figure~\ref{fig:usage}) shows CPU and GPU usage by group and user, plus job submission patterns as a heatmap. Interactive computing sessions (Figure~\ref{fig:interactive}) display active RStudio and Jupyter sessions. The infrastructure monitoring view (Figure~\ref{fig:infrastructure}) extends visibility to departmental workstations and storage servers.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/usage-analytics.png}
\caption{Usage analytics views. \textbf{Top}: Resource usage summary showing CPU-hours, GPU-hours, and breakdown by group and user. \textbf{Bottom}: Activity heatmap displaying job submissions by day and hour, revealing usage patterns.}
\label{fig:usage}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/interactive-sessions.png}
\caption{Interactive computing sessions dashboard showing active RStudio and Jupyter sessions with memory usage, session age, and status.}
\label{fig:interactive}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{figures/infrastructure.png}
\caption{Infrastructure monitoring views. \textbf{Top}: Workstation overview showing departmental machines (Biology, Chemistry, Physics, Math/CS) with status, CPU load, memory, disk usage, and logged-in users. \textbf{Bottom}: Storage overview displaying NFS servers with capacity, usage, ZFS pool health, and connected clients.}
\label{fig:infrastructure}
\end{figure}
\subsection*{Job Similarity Network}
A distinguishing feature of NØMAD is the job similarity network visualization (Figure~\ref{fig:network}). Jobs are represented as nodes in a 3D space defined by their I/O characteristics: NFS write ratio, local write volume, and I/O wait percentage. When full job metrics are available, edges connect jobs with cosine similarity $\geq 0.7$ based on their feature vectors.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/network.png}
\caption{Job similarity network visualization with feature variance analysis. The 3D view shows jobs colored by outcome (green=completed, red=failed, orange=timeout, purple=OOM). The left panel displays feature variance statistics sorted by coefficient of variation (CV); high CV values for exit\_signal and failure\_reason (exceeding 200\%) reflect normal cluster operation where most jobs succeed. The legend shows job outcome distribution across 1000 jobs.}
\label{fig:network}
\end{figure}

The feature variance panel helps administrators verify normal cluster operation and identify which metrics contribute most to failure prediction.

\subsection*{Quality Control}
NØMAD includes a comprehensive test suite covering unit tests for collectors, feature engineering, and ML components; integration tests for the full data pipeline; a demo mode (\texttt{nomad demo}) that generates synthetic HPC data for testing without requiring a real cluster; and continuous integration via GitHub Actions.

The software has been tested on clusters ranging from 6 to 200 nodes, including both CPU-only and GPU-enabled partitions. The codebase follows PEP 8 style guidelines and includes type hints for improved maintainability.

\subsection*{Educational Analytics}

NØMAD includes an educational analytics module (\texttt{nomad edu}) designed
for classroom instruction, research mentorship, and HPC training programs. The
module tracks the development of computational proficiency over time by
analyzing per-job behavioral fingerprints across five dimensions: CPU
efficiency, memory sizing, time estimation, I/O awareness, and GPU utilization.

\textbf{Job Explanation} (\texttt{nomad edu explain}): Translates raw job data
into plain-language feedback. For each completed job, the system provides
dimension-specific scores (0--100), proficiency levels (Excellent, Good,
Developing, Needs Work), and actionable recommendations. For example, a job
with 21\% CPU utilization receives the feedback: ``Very low CPU utilization---
requested 4 cores but used $\sim$1. Try: \texttt{\#SBATCH --ntasks=1}.''

\begin{figure}[htbp]
\centering
\includegraphics[width=0.45\textwidth]{figures/edu_explain.png}\hspace{0.5cm}\includegraphics[width=0.45\textwidth]{figures/edu_trajectory.png}

\vspace{0.3cm}

\includegraphics[width=0.45\textwidth]{figures/edu_report.png}

\vspace{0.2cm}


\caption{Educational analytics outputs. \textbf{Top left} (\texttt{nomad edu explain 1104}): Job explanation
with proficiency scores and recommendations. \textbf{Top right} (\texttt{nomad edu trajectory alice}): User trajectory
tracking improvement over 173 jobs. \textbf{Bottom} (\texttt{nomad edu report cs101}): Group report for a course
section with per-student breakdown.}
\label{fig:edu}
\end{figure}

\textbf{User Trajectories} (\texttt{nomad edu trajectory}): Tracks an
individual's improvement across their job submissions. The system computes
sliding-window averages to identify trends (improving, stable, declining) for
each proficiency dimension, enabling mentors to provide targeted guidance.

\textbf{Group Reports} (\texttt{nomad edu report}): Aggregates proficiency
data across course sections or research groups. Instructors can generate
summaries such as ``15 of 20 students improved memory efficiency over the
semester'' or identify that a majority of students struggle with I/O patterns,
suggesting curriculum adjustments.

These features address a common challenge in HPC education: students submit
jobs that technically complete but exhibit significant resource waste or
suboptimal patterns. Traditional approaches require manual inspection of
\texttt{sacct} output, which is impractical for classes with hundreds of
students. NØMAD automates this assessment while providing pedagogically useful
feedback that helps students develop genuine computational proficiency rather
than simply learning to avoid errors.

Proficiency scores are persisted to the database, enabling longitudinal studies
of HPC training effectiveness and research into factors that accelerate skill
development.

\section*{(2) Availability}

\subsection*{Operating System}
Linux (tested on Ubuntu 22.04, Rocky Linux 9, CentOS 7)

\subsection*{Programming Language}
Python 3.9+

\subsection*{Additional System Requirements}
\begin{itemize}
\item SLURM workload manager (optional, enables job-level analytics)
\item sysstat package (\texttt{iostat}, \texttt{mpstat}) for system metrics
\item Network access for dashboard (default port 5000)
\item Minimal disk space for SQLite database ($\sim$1MB per 10,000 samples)
\end{itemize}

\subsection*{Dependencies}
\begin{itemize}
\item click $\geq$ 8.0
\item toml $\geq$ 0.10
\item numpy $\geq$ 1.21
\item pandas $\geq$ 1.3
\item scipy $\geq$ 1.7
\item scikit-learn \cite{sklearn} (optional, for ML features)
\item torch (optional, for GNN/LSTM models)
\item torch-geometric (optional, for graph neural networks)
\end{itemize}

\subsection*{List of Contributors}
\begin{enumerate}
\item Tonini, João Filipe Riva (Lead developer)
\end{enumerate}

\subsection*{Software Location}

\subsubsection*{Archive}
\begin{tabular}{ll}
Name: & Zenodo \\
Persistent identifier: & \url{https://doi.org/10.5281/zenodo.18614517} \\
Licence: & AGPL-3.0-or-later \\
Publisher: & João Filipe Riva Tonini \\
Version: & 1.2.2 \\
Date published: & February 2026 \\
\end{tabular}

\subsubsection*{Code Repository}
\begin{tabular}{ll}
Name: & GitHub \\
Identifier: & \url{https://github.com/jtonini/nomad-hpc} \\
Licence: & AGPL-3.0-or-later \\
Date published: & December 2025 \\
\end{tabular}


\subsubsection*{Package Repository}
\begin{tabular}{ll}
Name: & PyPI \\
Identifier: & \url{https://pypi.org/project/nomad-hpc/} \\
Installation: & \texttt{pip install nomad-hpc} \\
\end{tabular}

\subsubsection*{Project Website}
\begin{tabular}{ll}
URL: & \url{https://nomad-hpc.com} \\
Documentation: & \url{https://jtonini.github.io/nomad-hpc/} \\
\end{tabular}
\subsection*{Language}
English

\section*{(3) Reuse Potential}

NØMAD is designed for broad reuse across HPC environments of varying scales. The software can be deployed in several contexts:

\textbf{Small Research Groups}: Groups managing individual workstations or small clusters can use NØMAD's zero-infrastructure design to implement sophisticated monitoring without enterprise tools. The \texttt{nomad demo} mode allows evaluation without requiring existing HPC infrastructure.

\textbf{Medium HPC Centers}: University and institutional HPC centers can deploy NØMAD as either a primary monitoring solution or as a complement to existing tools like Grafana or Nagios, adding predictive capabilities to traditional threshold-based alerting.

\textbf{Research Applications}: The similarity network approach provides a
framework for studying HPC failure patterns. The network structure, combined
with job-level proficiency scores, enables quantitative analysis of resource
usage patterns and their relationship to job outcomes.

\textbf{Community Data Sharing}: The \texttt{nomad community export} command
generates anonymized datasets suitable for cross-institutional research. Export
formats include:
\begin{itemize}
\item \textbf{Job fingerprints}: Feature vectors with hashed user/job identifiers
\item \textbf{Failure patterns}: Aggregated statistics on failure modes by resource profile
\item \textbf{Proficiency distributions}: Anonymized skill development trajectories
\end{itemize}
These exports enable collaborative research on HPC usage patterns without
exposing sensitive institutional data. Researchers can study questions such as:
Which failure modes are universal vs.\ site-specific? Do proficiency
improvement rates vary by discipline? What resource configurations correlate
with job success across different cluster architectures?

The anonymization process removes usernames, job names, and absolute timestamps
while preserving the relational structure needed for network-based analysis.
Exported data uses relative time offsets and hashed identifiers, making it
suitable for publication as supplementary materials or inclusion in shared
research datasets.

\textbf{Educational Use}: Beyond the demo mode, NØMAD provides a complete
educational analytics pipeline. Instructors can assign Linux groups to course
sections (e.g., \texttt{cs101}, \texttt{bio301}) and use \texttt{nomad edu
report} to track class-wide proficiency development. Individual students
receive automated feedback via \texttt{nomad edu explain}, reducing instructor
workload while providing immediate, actionable guidance. The module is
particularly valuable for:
\begin{itemize}
\item \textbf{HPC workshops}: Pre/post assessment of participant skills
\item \textbf{Research onboarding}: Tracking new graduate students' development
\item \textbf{Classroom instruction}: Automated grading of computational assignments
\item \textbf{Self-directed learning}: Students can independently review their job history
\end{itemize}

\subsection*{Extension Points}
Developers can extend NØMAD in several ways:
\begin{itemize}
\item \textbf{Custom Collectors}: Add new metric sources by implementing the collector interface
\item \textbf{Alert Channels}: Integrate additional notification services beyond email/Slack/webhooks
\item \textbf{ML Models}: Train site-specific models using the provided feature engineering pipeline
\item \textbf{Dashboard Widgets}: Extend the web dashboard with custom visualizations
\end{itemize}

\subsection*{Support}
\begin{itemize}
\item GitHub Issues: \url{https://github.com/jtonini/nomad-hpc/issues}
\item Email: jtonini@richmond.edu
\item Documentation: \url{https://jtonini.github.io/nomad-hpc/}
\end{itemize}

Users and developers are welcome to submit bug reports, feature requests, and pull requests via GitHub.

\section*{Acknowledgements}
The author thanks George Flanagin for advice and inspiration on HPC system administration, and the University of Richmond's Office of the Provost for financial and resource support.

\section*{Funding Statement}
This work was supported by the University of Richmond.

\section*{Competing Interests}
The author declares no competing interests.

\begin{thebibliography}{10}

\bibitem{slurm}
Yoo, A.B., Jette, M.A., Grondona, M. (2003). SLURM: Simple Linux Utility for Resource Management. In: Job Scheduling Strategies for Parallel Processing, Lecture Notes in Computer Science, vol. 2862, Springer, pp. 44--60. DOI: \url{https://doi.org/10.1007/10968987_3}

\bibitem{prometheus}
Prometheus Authors (2012--present). Prometheus: From metrics to insight.
URL: \url{https://prometheus.io/}

\bibitem{grafana}
Grafana Labs (2014--present). Grafana: The open observability platform.
URL: \url{https://grafana.com/}

\bibitem{nagios}
Galstad, E. (1999--present). Nagios: The industry standard in IT
infrastructure monitoring. URL: \url{https://www.nagios.org/}

\bibitem{tacc}
Evans, R.T., Browne, J.C., Barth, W.L. (2014). Comprehensive resource use monitoring for HPC systems with TACC Stats. In: Proceedings of the First International Workshop on HPC User Support Tools, IEEE, pp. 13--21. DOI: \url{https://doi.org/10.1109/SC.2014.18}

\bibitem{xdmod}
Palmer, J.T., Gallo, S.M., Furlani, T.R., Jones, M.D., DeLeon, R.L., White, J.P., Simakov, N., Patra, A.K., Sperhac, J., Yearke, T., Rathsam, R., Inber, M., Guillen, O., Cornelius, C.D. (2015). Open XDMoD: A tool for the comprehensive management of high-performance computing resources. Computing in Science \& Engineering 17(4):52--62. DOI: \url{https://doi.org/10.1109/MCSE.2015.32}

\bibitem{ldms}
Agelastos, A., Allan, B., Brandt, J., Cassella, P., Enos, J., Fullop, J., Gentile, A., Monk, S., Naksinehaboon, N., Ogden, J., Rajan, M., Showerman, M., Stevenson, J., Taerat, N., Tucker, T. (2014). The Lightweight Distributed Metric Service: A scalable infrastructure for continuous monitoring of large scale computing systems and applications. In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, IEEE, pp. 154--165. DOI: \url{https://doi.org/10.1145/2616498.2616533}

\bibitem{mlhpc}
Tuncer, O., Ates, E., Zhang, Y., Turber, A., Brandt, J., Leung, V.J.,
Egele, M., Coskun, A.K. (2017). Diagnosing performance variations in HPC
applications using machine learning. In: High Performance Computing,
Lecture Notes in Computer Science, vol. 10266, Springer, pp. 355--373.
DOI: \url{https://doi.org/10.1007/978-3-319-58667-0_19}

\bibitem{vilhena}
Vilhena, D.A., Antonelli, A. (2015). A network approach for identifying
and delimiting biogeographical regions. Nature Communications 6:6848.
DOI: \url{https://doi.org/10.1038/ncomms7848}

\bibitem{sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12:2825--2830. URL: \url{http://jmlr.org/papers/v12/pedregosa11a.html}

\end{thebibliography}

\end{document}
